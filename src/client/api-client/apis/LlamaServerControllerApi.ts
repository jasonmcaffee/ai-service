/* tslint:disable */
/* eslint-disable */
/**
 * AI API
 * API to manage to interact with AI
 *
 * The version of the OpenAPI document: 1.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import * as runtime from '../runtime';
import type {
  LlamaCppModelsResponse,
  LoadModelRequest,
} from '../models/index';
import {
    LlamaCppModelsResponseFromJSON,
    LlamaCppModelsResponseToJSON,
    LoadModelRequestFromJSON,
    LoadModelRequestToJSON,
} from '../models/index';

export interface GetCurrentlyRunningModelRequest {
    url: string;
}

export interface LoadModelOperationRequest {
    loadModelRequest: LoadModelRequest;
}

/**
 * 
 */
export class LlamaServerControllerApi extends runtime.BaseAPI {

    /**
     * get currently running model information
     */
    async getCurrentlyRunningModelRaw(requestParameters: GetCurrentlyRunningModelRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<LlamaCppModelsResponse>> {
        if (requestParameters['url'] == null) {
            throw new runtime.RequiredError(
                'url',
                'Required parameter "url" was null or undefined when calling getCurrentlyRunningModel().'
            );
        }

        const queryParameters: any = {};

        if (requestParameters['url'] != null) {
            queryParameters['url'] = requestParameters['url'];
        }

        const headerParameters: runtime.HTTPHeaders = {};

        const response = await this.request({
            path: `/llamaServerController/currentlyRunningModel`,
            method: 'GET',
            headers: headerParameters,
            query: queryParameters,
        }, initOverrides);

        return new runtime.JSONApiResponse(response, (jsonValue) => LlamaCppModelsResponseFromJSON(jsonValue));
    }

    /**
     * get currently running model information
     */
    async getCurrentlyRunningModel(url: string, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<LlamaCppModelsResponse> {
        const response = await this.getCurrentlyRunningModelRaw({ url: url }, initOverrides);
        return await response.value();
    }

    /**
     * Load and start the LLM model server
     */
    async loadModelRaw(requestParameters: LoadModelOperationRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<runtime.ApiResponse<void>> {
        if (requestParameters['loadModelRequest'] == null) {
            throw new runtime.RequiredError(
                'loadModelRequest',
                'Required parameter "loadModelRequest" was null or undefined when calling loadModel().'
            );
        }

        const queryParameters: any = {};

        const headerParameters: runtime.HTTPHeaders = {};

        headerParameters['Content-Type'] = 'application/json';

        const response = await this.request({
            path: `/llamaServerController/loadModel`,
            method: 'POST',
            headers: headerParameters,
            query: queryParameters,
            body: LoadModelRequestToJSON(requestParameters['loadModelRequest']),
        }, initOverrides);

        return new runtime.VoidApiResponse(response);
    }

    /**
     * Load and start the LLM model server
     */
    async loadModel(loadModelRequest: LoadModelRequest, initOverrides?: RequestInit | runtime.InitOverrideFunction): Promise<void> {
        await this.loadModelRaw({ loadModelRequest: loadModelRequest }, initOverrides);
    }

}
